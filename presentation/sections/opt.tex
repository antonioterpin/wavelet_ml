\subsection{Optimization}
\begin{frame}
    \frametitle{Optimization}
    \onslide <1-> {
        \begin{block}{Bayesian optimization}
            Bayesian optimization aims to solve an optimization problem $\max\limits_{x\in A}f\left(x\right)$ when:
            \begin{itemize}
                \item $A \subset \mathbb{R}^d$ with $d$ not too large.
                \item Easy to assess $A$ membership.
                \item $f$ can be modeled using \emph{Gaussian process regression}.
                \item $f$ is a \emph{black box}.
            \end{itemize}
        \end{block}
    }
    \onslide <2-> {
        \begin{exampleblock}{Loss function}
            \begin{equation*}
                J\left(\alpha\right) = \frac{2\lvert X \cap Y \rvert}{\lvert X \rvert + \lvert Y \rvert} = 1-f(x)
            \end{equation*}
        \end{exampleblock}
    }
\end{frame}

\begin{frame}
    \frametitle{Optimization}
    \vskip -0.5cm
    \begin{block}{Bayesian optimization}
        Firstly a collection of points $x_1, \ldots, x_k \in \mathbb{R}^d$ is considered. Call $\underline{b}$ this collection. Suppose $f$ can be modeled as:
        \begin{equation*}
            f\left(\underline{b}\right) \sim \mathcal{N}\left(\underline{\mu}_b, \underline{\underline{\Sigma}}_{bb}\right)
        \end{equation*}
        The probability distribution of a $f$ on a second collection of points of $\mathbb{R}^d$ $\underline{a} = \left[x_{k + 1} \ldots x_{n}\right]$ can be inferred as:
        \begin{equation*}
            f\left(\underline{a}\right) \mid f\left(\underline{b}\right) \sim \mathcal{N}\left(\underline{\mu}, \underline{\underline{\Sigma}}\right)
        \end{equation*}
        With:
        \begin{equation*}
            \underline{\mu} = \underline{\mu}_{a} + \underline{\underline{\Sigma}}_{ab}\underline{\underline{\Sigma}}_{bb}^{-1}\left(\underline{b} - \underline{\mu}_b\right)
        \end{equation*}
        \begin{equation*}
            \underline{\underline{\Sigma}}^2 = \underline{\underline{\Sigma}}_{aa} - \underline{\underline{\Sigma}}_{ab}\underline{\underline{\Sigma}}_{bb}^{-1}\underline{\underline{\Sigma}}_{ba}
        \end{equation*}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Optimization}
    \begin{exampleblock}{Acquisition function}
        Therefore, it is possible to consider an \emph{acquisition function}, i.e. a function that given the previous $x_1, \ldots, x_n$ values determines the best choice of $x_{n+1}$:
        \begin{equation*}
            a \colon \left[x_1 \ldots x_n\right] \rightarrow x_{n+1} \vert \mathcal{P}\left[f(x_{n+1}) = \max\limits_{x \in A} f\left(x\right)\right] \text{is max}
        \end{equation*}
    \end{exampleblock}
\end{frame}